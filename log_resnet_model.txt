W0730 14:42:28.226235 139713236703104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

W0730 14:42:28.370960 139713236703104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 13, 13, 64)   9472        input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 13, 13, 64)   256         conv2d_34[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 6, 6, 64)     36928       max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 6, 6, 64)     256         conv2d_35[0][0]                  
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 6, 6, 64)     36928       batch_normalization_30[0][0]     
__________________________________________________________________________________________________
add_14 (Add)                    (None, 6, 6, 64)     0           conv2d_36[0][0]                  
                                                                 max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 6, 6, 64)     36928       add_14[0][0]                     
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 6, 6, 64)     256         conv2d_37[0][0]                  
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 6, 6, 64)     36928       batch_normalization_31[0][0]     
__________________________________________________________________________________________________
add_15 (Add)                    (None, 6, 6, 64)     0           conv2d_38[0][0]                  
                                                                 add_14[0][0]                     
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 6, 6, 64)     36928       add_15[0][0]                     
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 6, 6, 64)     256         conv2d_39[0][0]                  
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 6, 6, 64)     36928       batch_normalization_32[0][0]     
__________________________________________________________________________________________________
add_16 (Add)                    (None, 6, 6, 64)     0           conv2d_40[0][0]                  
                                                                 add_15[0][0]                     
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 3, 3, 128)    73856       add_16[0][0]                     
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 3, 3, 128)    512         conv2d_41[0][0]                  
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 3, 3, 128)    147584      batch_normalization_33[0][0]     
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 3, 3, 128)    512         conv2d_42[0][0]                  
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 3, 3, 128)    147584      batch_normalization_34[0][0]     
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 3, 3, 128)    512         conv2d_43[0][0]                  
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 3, 3, 128)    147584      batch_normalization_35[0][0]     
__________________________________________________________________________________________________
add_17 (Add)                    (None, 3, 3, 128)    0           conv2d_44[0][0]                  
                                                                 batch_normalization_34[0][0]     
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 3, 3, 128)    147584      add_17[0][0]                     
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 3, 3, 128)    512         conv2d_45[0][0]                  
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 3, 3, 128)    147584      batch_normalization_36[0][0]     
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 3, 3, 128)    512         conv2d_46[0][0]                  
__________________________________________________________________________________________________
add_18 (Add)                    (None, 3, 3, 128)    0           batch_normalization_37[0][0]     
                                                                 add_17[0][0]                     
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 3, 3, 128)    147584      add_18[0][0]                     
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 3, 3, 128)    512         conv2d_47[0][0]                  
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 3, 3, 128)    147584      batch_normalization_38[0][0]     
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 3, 3, 128)    512         conv2d_48[0][0]                  
__________________________________________________________________________________________________
add_19 (Add)                    (None, 3, 3, 128)    0           batch_normalization_39[0][0]     
                                                                 conv2d_47[0][0]                  
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 2, 2, 256)    295168      add_19[0][0]                     
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 2, 2, 256)    1024        conv2d_49[0][0]                  
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 2, 2, 256)    590080      batch_normalization_40[0][0]     
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 2, 2, 256)    1024        conv2d_50[0][0]                  
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 2, 2, 256)    590080      batch_normalization_41[0][0]     
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 2, 2, 256)    1024        conv2d_51[0][0]                  
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 2, 2, 256)    590080      batch_normalization_42[0][0]     
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 2, 2, 256)    1024        conv2d_52[0][0]                  
__________________________________________________________________________________________________
add_20 (Add)                    (None, 2, 2, 256)    0           batch_normalization_41[0][0]     
                                                                 batch_normalization_43[0][0]     
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 2, 2, 256)    590080      add_20[0][0]                     
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 2, 2, 256)    1024        conv2d_53[0][0]                  
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 2, 2, 256)    590080      batch_normalization_44[0][0]     
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 2, 2, 256)    1024        conv2d_54[0][0]                  
__________________________________________________________________________________________________
add_21 (Add)                    (None, 2, 2, 256)    0           add_20[0][0]                     
                                                                 batch_normalization_45[0][0]     
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 2, 2, 256)    590080      add_21[0][0]                     
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 2, 2, 256)    1024        conv2d_55[0][0]                  
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 2, 2, 256)    590080      batch_normalization_46[0][0]     
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 2, 2, 256)    1024        conv2d_56[0][0]                  
__________________________________________________________________________________________________
add_22 (Add)                    (None, 2, 2, 256)    0           add_21[0][0]                     
                                                                 batch_normalization_47[0][0]     
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 2, 2, 256)    590080      add_22[0][0]                     
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 2, 2, 256)    1024        conv2d_57[0][0]                  
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 2, 2, 256)    590080      batch_normalization_48[0][0]     
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 2, 2, 256)    1024        conv2d_58[0][0]                  
__________________________________________________________________________________________________
add_23 (Add)                    (None, 2, 2, 256)    0           add_22[0][0]                     
                                                                 batch_normalization_49[0][0]     
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 1, 1, 512)    1180160     add_23[0][0]                     
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 1, 1, 512)    2048        conv2d_59[0][0]                  
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 1, 1, 512)    2359808     batch_normalization_50[0][0]     
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 1, 1, 512)    2048        conv2d_60[0][0]                  
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 1, 1, 512)    2359808     batch_normalization_51[0][0]     
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 1, 1, 512)    2048        conv2d_61[0][0]                  
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 1, 1, 512)    2359808     batch_normalization_52[0][0]     
__________________________________________________________________________________________________
add_24 (Add)                    (None, 1, 1, 512)    0           conv2d_60[0][0]                  
                                                                 conv2d_62[0][0]                  
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 1, 1, 512)    2359808     add_24[0][0]                     
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 1, 1, 512)    2048        conv2d_63[0][0]                  
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 1, 1, 512)    2359808     batch_normalization_53[0][0]     
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 1, 1, 512)    2048        conv2d_64[0][0]                  
__________________________________________________________________________________________________
add_25 (Add)                    (None, 1, 1, 512)    0           add_24[0][0]                     
                                                                 batch_normalization_54[0][0]     
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 1, 1, 512)    2359808     add_25[0][0]                     
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 1, 1, 512)    2048        conv2d_65[0][0]                  
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 1, 1, 512)    2359808     batch_normalization_55[0][0]     
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 1, 1, 512)    2048        conv2d_66[0][0]                  
__________________________________________________________________________________________________
add_26 (Add)                    (None, 1, 1, 512)    0           add_25[0][0]                     
                                                                 batch_normalization_56[0][0]     
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 512)          0           add_26[0][0]                     
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 10)           5130        flatten_2[0][0]                  
==================================================================================================
Total params: 24,677,002
Trainable params: 24,662,410
Non-trainable params: 14,592
__________________________________________________________________________________________________
Epoch 1/100
1407/1406 [==============================] - 237s 169ms/step - loss: 2.6581 - acc: 0.1756 - val_loss: 3.5395 - val_acc: 0.1244
Epoch 2/100
1407/1406 [==============================] - 225s 160ms/step - loss: 2.6581 - acc: 0.1316 - val_loss: 2.8128 - val_acc: 0.1062
Epoch 3/100
1407/1406 [==============================] - 224s 159ms/step - loss: 2.3330 - acc: 0.1567 - val_loss: 2.2559 - val_acc: 0.1854
Epoch 4/100
1407/1406 [==============================] - 224s 160ms/step - loss: 2.0783 - acc: 0.2058 - val_loss: 1.9920 - val_acc: 0.2666
Epoch 5/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.9422 - acc: 0.2504 - val_loss: 1.9030 - val_acc: 0.2960
Epoch 6/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.8401 - acc: 0.2950 - val_loss: 1.8909 - val_acc: 0.3420
Epoch 7/100
1407/1406 [==============================] - 225s 160ms/step - loss: 1.7623 - acc: 0.3283 - val_loss: 1.6082 - val_acc: 0.3516
Epoch 8/100
1407/1406 [==============================] - 225s 160ms/step - loss: 1.6806 - acc: 0.3639 - val_loss: 1.6290 - val_acc: 0.4228
Epoch 9/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.6134 - acc: 0.3990 - val_loss: 1.4634 - val_acc: 0.4510
Epoch 10/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.5283 - acc: 0.4421 - val_loss: 1.8098 - val_acc: 0.4954
Epoch 11/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.4509 - acc: 0.4812 - val_loss: 1.3638 - val_acc: 0.5236
Epoch 12/100
1407/1406 [==============================] - 223s 159ms/step - loss: 1.3788 - acc: 0.5065 - val_loss: 1.2697 - val_acc: 0.5438
Epoch 13/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.3189 - acc: 0.5319 - val_loss: 1.0871 - val_acc: 0.6180
Epoch 14/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.2651 - acc: 0.5512 - val_loss: 1.0296 - val_acc: 0.6328
Epoch 15/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.2338 - acc: 0.5678 - val_loss: 1.2895 - val_acc: 0.5862
Epoch 16/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.1921 - acc: 0.5837 - val_loss: 1.2441 - val_acc: 0.6384
Epoch 17/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.1529 - acc: 0.5968 - val_loss: 1.0291 - val_acc: 0.6504
Epoch 18/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.1371 - acc: 0.6049 - val_loss: 1.1421 - val_acc: 0.6410
Epoch 19/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.0891 - acc: 0.6211 - val_loss: 1.2511 - val_acc: 0.6416
Epoch 20/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.0734 - acc: 0.6277 - val_loss: 0.9202 - val_acc: 0.6982
Epoch 21/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.0522 - acc: 0.6353 - val_loss: 1.1400 - val_acc: 0.6692
Epoch 22/100
1407/1406 [==============================] - 224s 159ms/step - loss: 1.0288 - acc: 0.6452 - val_loss: 1.0537 - val_acc: 0.6794
Epoch 23/100
1407/1406 [==============================] - 225s 160ms/step - loss: 1.0111 - acc: 0.6506 - val_loss: 1.0130 - val_acc: 0.6992
Epoch 24/100
1407/1406 [==============================] - 224s 159ms/step - loss: 0.9897 - acc: 0.6580 - val_loss: 0.9959 - val_acc: 0.7076
Epoch 25/100
1407/1406 [==============================] - 224s 159ms/step - loss: 0.9805 - acc: 0.6636 - val_loss: 0.9317 - val_acc: 0.7166
Epoch 26/100
1407/1406 [==============================] - 224s 159ms/step - loss: 0.9701 - acc: 0.6672 - val_loss: 1.2166 - val_acc: 0.6842
Epoch 27/100
1407/1406 [==============================] - 224s 159ms/step - loss: 0.9527 - acc: 0.6722 - val_loss: 1.0865 - val_acc: 0.6880
Epoch 28/100
1407/1406 [==============================] - 224s 160ms/step - loss: 0.9305 - acc: 0.6823 - val_loss: 0.9451 - val_acc: 0.7102
Epoch 29/100
1407/1406 [==============================] - 224s 159ms/step - loss: 0.9243 - acc: 0.6845 - val_loss: 0.8936 - val_acc: 0.7378
Epoch 30/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.9222 - acc: 0.6855 - val_loss: 0.9970 - val_acc: 0.7268
Epoch 31/100
1407/1406 [==============================] - 224s 159ms/step - loss: 0.8930 - acc: 0.6963 - val_loss: 1.1162 - val_acc: 0.7040
Epoch 32/100
1407/1406 [==============================] - 224s 160ms/step - loss: 0.8844 - acc: 0.6988 - val_loss: 0.9655 - val_acc: 0.7224
Epoch 33/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.8762 - acc: 0.7033 - val_loss: 0.9792 - val_acc: 0.7284
Epoch 34/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.8691 - acc: 0.7042 - val_loss: 0.9029 - val_acc: 0.7432
Epoch 35/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.8562 - acc: 0.7098 - val_loss: 1.0931 - val_acc: 0.7182
Epoch 36/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.8544 - acc: 0.7107 - val_loss: 0.8664 - val_acc: 0.7494
Epoch 37/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.8384 - acc: 0.7144 - val_loss: 0.9130 - val_acc: 0.7478
Epoch 38/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.8322 - acc: 0.7157 - val_loss: 0.8563 - val_acc: 0.7480
Epoch 39/100
1407/1406 [==============================] - 224s 160ms/step - loss: 0.8223 - acc: 0.7233 - val_loss: 0.9276 - val_acc: 0.7358
Epoch 40/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.8177 - acc: 0.7245 - val_loss: 0.9413 - val_acc: 0.7374
Epoch 41/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.8010 - acc: 0.7289 - val_loss: 0.9372 - val_acc: 0.7400
Epoch 42/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7989 - acc: 0.7300 - val_loss: 1.0923 - val_acc: 0.7446
Epoch 43/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7854 - acc: 0.7340 - val_loss: 0.8558 - val_acc: 0.7564
Epoch 44/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7819 - acc: 0.7347 - val_loss: 0.9159 - val_acc: 0.7574
Epoch 45/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7715 - acc: 0.7364 - val_loss: 0.9904 - val_acc: 0.7500
Epoch 46/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7628 - acc: 0.7422 - val_loss: 0.7702 - val_acc: 0.7714
Epoch 47/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7630 - acc: 0.7433 - val_loss: 0.9020 - val_acc: 0.7578
Epoch 48/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7542 - acc: 0.7439 - val_loss: 0.7439 - val_acc: 0.7820
Epoch 49/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7487 - acc: 0.7496 - val_loss: 0.6928 - val_acc: 0.7848
Epoch 50/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7416 - acc: 0.7488 - val_loss: 0.8985 - val_acc: 0.7694
Epoch 51/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7352 - acc: 0.7516 - val_loss: 0.8948 - val_acc: 0.7672
Epoch 52/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7209 - acc: 0.7542 - val_loss: 0.7551 - val_acc: 0.7736
Epoch 53/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7261 - acc: 0.7535 - val_loss: 0.7090 - val_acc: 0.7900
Epoch 54/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7148 - acc: 0.7592 - val_loss: 0.8240 - val_acc: 0.7798
Epoch 55/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7105 - acc: 0.7601 - val_loss: 0.7558 - val_acc: 0.7822
Epoch 56/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7082 - acc: 0.7624 - val_loss: 0.8581 - val_acc: 0.7770
Epoch 57/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.7039 - acc: 0.7623 - val_loss: 0.7474 - val_acc: 0.7724
Epoch 58/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.6929 - acc: 0.7655 - val_loss: 0.8369 - val_acc: 0.7720
Epoch 59/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.6931 - acc: 0.7644 - val_loss: 0.8668 - val_acc: 0.7758
Epoch 60/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.6869 - acc: 0.7678 - val_loss: 0.7368 - val_acc: 0.7752
Epoch 61/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.6818 - acc: 0.7708 - val_loss: 0.7667 - val_acc: 0.7878
Epoch 62/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.6794 - acc: 0.7691 - val_loss: 0.8228 - val_acc: 0.7660
Epoch 63/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.6719 - acc: 0.7711 - val_loss: 0.8836 - val_acc: 0.7786
Epoch 64/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.6630 - acc: 0.7766 - val_loss: 0.8376 - val_acc: 0.7810
Epoch 65/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.6589 - acc: 0.7771 - val_loss: 0.7269 - val_acc: 0.7764
Epoch 66/100
1407/1406 [==============================] - 224s 159ms/step - loss: 0.6582 - acc: 0.7772 - val_loss: 0.7739 - val_acc: 0.7824
Epoch 67/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.6561 - acc: 0.7764 - val_loss: 0.7600 - val_acc: 0.7850
Epoch 68/100
1407/1406 [==============================] - 224s 160ms/step - loss: 0.6587 - acc: 0.7770 - val_loss: 0.7968 - val_acc: 0.7842
Epoch 69/100
1407/1406 [==============================] - 224s 159ms/step - loss: 0.6511 - acc: 0.7767 - val_loss: 0.7649 - val_acc: 0.7802
Epoch 70/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.6429 - acc: 0.7823 - val_loss: 0.8319 - val_acc: 0.7616
Epoch 71/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.6419 - acc: 0.7831 - val_loss: 0.7016 - val_acc: 0.7828
Epoch 72/100
1407/1406 [==============================] - 225s 160ms/step - loss: 0.6381 - acc: 0.7843 - val_loss: 0.8432 - val_acc: 0.7886
Epoch 73/100
1407/1406 [==============================] - 226s 161ms/step - loss: 0.6324 - acc: 0.7853 - val_loss: 0.6896 - val_acc: 0.8036
Epoch 74/100
1407/1406 [==============================] - 226s 161ms/step - loss: 0.6369 - acc: 0.7850 - val_loss: 0.6618 - val_acc: 0.7940
Epoch 75/100
1407/1406 [==============================] - 226s 161ms/step - loss: 0.6273 - acc: 0.7854 - val_loss: 0.6537 - val_acc: 0.7980
Epoch 76/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.6186 - acc: 0.7893 - val_loss: 0.7255 - val_acc: 0.7880
Epoch 77/100
1407/1406 [==============================] - 226s 161ms/step - loss: 0.6230 - acc: 0.7883 - val_loss: 0.6054 - val_acc: 0.8112
Epoch 78/100
1407/1406 [==============================] - 226s 161ms/step - loss: 0.6143 - acc: 0.7924 - val_loss: 0.6188 - val_acc: 0.7994
Epoch 79/100
1407/1406 [==============================] - 227s 162ms/step - loss: 0.6158 - acc: 0.7910 - val_loss: 0.6880 - val_acc: 0.7972
Epoch 80/100
1407/1406 [==============================] - 227s 162ms/step - loss: 0.6139 - acc: 0.7912 - val_loss: 0.7141 - val_acc: 0.7824
Epoch 81/100
1407/1406 [==============================] - 228s 162ms/step - loss: 0.6090 - acc: 0.7931 - val_loss: 0.6507 - val_acc: 0.8078
Epoch 82/100
1407/1406 [==============================] - 228s 162ms/step - loss: 0.6100 - acc: 0.7922 - val_loss: 0.5988 - val_acc: 0.8124
Epoch 83/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.6002 - acc: 0.7952 - val_loss: 0.7444 - val_acc: 0.7912
Epoch 84/100
1407/1406 [==============================] - 227s 162ms/step - loss: 0.5969 - acc: 0.7990 - val_loss: 0.6184 - val_acc: 0.8026
Epoch 85/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.5945 - acc: 0.7976 - val_loss: 0.7239 - val_acc: 0.7954
Epoch 86/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.5891 - acc: 0.8015 - val_loss: 0.7271 - val_acc: 0.8002
Epoch 87/100
1407/1406 [==============================] - 227s 162ms/step - loss: 0.5943 - acc: 0.7975 - val_loss: 0.7468 - val_acc: 0.7956
Epoch 88/100
1407/1406 [==============================] - 228s 162ms/step - loss: 0.5935 - acc: 0.7960 - val_loss: 0.6764 - val_acc: 0.8020
Epoch 89/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.5883 - acc: 0.8004 - val_loss: 0.6324 - val_acc: 0.8102
Epoch 90/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.5825 - acc: 0.8028 - val_loss: 0.6609 - val_acc: 0.7966
Epoch 91/100
1407/1406 [==============================] - 228s 162ms/step - loss: 0.5818 - acc: 0.8022 - val_loss: 0.7327 - val_acc: 0.7918
Epoch 92/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.5811 - acc: 0.8009 - val_loss: 0.6924 - val_acc: 0.8100
Epoch 93/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.5874 - acc: 0.8021 - val_loss: 0.8442 - val_acc: 0.7906
Epoch 94/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.5766 - acc: 0.8048 - val_loss: 0.9224 - val_acc: 0.7808
Epoch 95/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.5741 - acc: 0.8058 - val_loss: 0.8912 - val_acc: 0.7888
Epoch 96/100
1407/1406 [==============================] - 226s 161ms/step - loss: 0.5742 - acc: 0.8052 - val_loss: 0.6645 - val_acc: 0.8054
Epoch 97/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.5631 - acc: 0.8080 - val_loss: 0.7899 - val_acc: 0.7884
Epoch 98/100
1407/1406 [==============================] - 227s 162ms/step - loss: 0.5710 - acc: 0.8065 - val_loss: 0.6197 - val_acc: 0.8116
Epoch 99/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.5706 - acc: 0.8060 - val_loss: 0.6358 - val_acc: 0.8214
Epoch 100/100
1407/1406 [==============================] - 227s 161ms/step - loss: 0.5619 - acc: 0.8077 - val_loss: 0.6876 - val_acc: 0.8032
10000/10000 [==============================] - 9s 875us/step
loss: 0.6971668257713318, acc: 0.7962
